{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA twitter for mining and NLP learning\n",
    "\n",
    "work in progress, based loosesly off of https://github.com/elaiken3/twitter_api-nlp-project1/blob/master/twitter_api-nlp-project.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import config_keys\n",
    "import twitter\n",
    "import re \n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "twitter_api = twitter.Api(\n",
    "    consumer_key         =   twitter_keys['consumer_key'],\n",
    "    consumer_secret      =   twitter_keys['consumer_secret'],\n",
    "    access_token_key     =   twitter_keys['access_token_key'],\n",
    "    access_token_secret  =   twitter_keys['access_token_secret'],\n",
    "    tweet_mode = 'extended'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TweetMiner function from Mike Roman\n",
    "\n",
    "class TweetMiner(object):\n",
    "\n",
    "    def __init__(self, api, result_limit = 20):\n",
    "        \n",
    "        self.api = api        \n",
    "        self.result_limit = result_limit\n",
    "        \n",
    "    def mine_user_tweets(self, user=\"HillaryClinton\", mine_retweets=False, max_pages=20):\n",
    "\n",
    "        data           =  []\n",
    "        last_tweet_id  =  False\n",
    "        page           =  1\n",
    "        \n",
    "        while page <= max_pages:\n",
    "            \n",
    "            if last_tweet_id:\n",
    "                statuses   =   self.api.GetUserTimeline(screen_name=user, count=self.result_limit, max_id=last_tweet_id - 1, include_rts=mine_retweets)\n",
    "                statuses = [ _.AsDict() for _ in statuses]\n",
    "            else:\n",
    "                statuses   =   self.api.GetUserTimeline(screen_name=user, count=self.result_limit, include_rts=mine_retweets)\n",
    "                statuses = [_.AsDict() for _ in statuses]\n",
    "                \n",
    "            for item in statuses:\n",
    "                # Using try except here.\n",
    "                # When retweets = 0 we get an error (GetUserTimeline fails to create a key, 'retweet_count')\n",
    "                try:\n",
    "                    mined = {\n",
    "                        'tweet_id':        item['id'],\n",
    "                        'handle':          item['user']['screen_name'],\n",
    "                        'retweet_count':   item['retweet_count'],\n",
    "                        'text':            item['full_text'],\n",
    "                        'mined_at':        datetime.datetime.now(),\n",
    "                        'created_at':      item['created_at'],\n",
    "                    }\n",
    "                \n",
    "                except:\n",
    "                        mined = {\n",
    "                        'tweet_id':        item['id'],\n",
    "                        'handle':          item['user']['screen_name'],\n",
    "                        'retweet_count':   0,\n",
    "                        'text':            item['full_text'],\n",
    "                        'mined_at':        datetime.datetime.now(),\n",
    "                        'created_at':      item['created_at'],\n",
    "                    }\n",
    "                \n",
    "                last_tweet_id = item['id']\n",
    "                data.append(mined)\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_embiid = api.GetUserTimeline(screen_name=\"JoelEmbiid\", count=20, include_rts=False)\n",
    "#tweets_embiid = [x.AsDict() for x in tweets_embiid]\n",
    "#for tweet in tweets_embiid:\n",
    "#    print tweet['id']\n",
    "#    print tweet['full_text']\n",
    "#    print'\\n'\n",
    "    \n",
    "# Result limit == count parameter from our GetUserTimeline()\n",
    "miner = TweetMiner(twitter_api, result_limit=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embiid = miner.mine_user_tweets(user=\"JoelEmbiid\")\n",
    "james = miner.mine_user_tweets(user=\"KingJames\")\n",
    "mccollum = miner.mine_user_tweets(user=\"CJMcCollum\")\n",
    "durant = miner.mine_user_tweets(user=\"KDTrey5\")\n",
    "#lillard = miner.mine_user_tweets(user=\"Dame_Lillard\")\n",
    "woj = miner.mine_user_tweets(user=\"wojespn\")\n",
    "shams = miner.mine_user_tweets(user=\"shamscharania\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_non_ascii(text):\n",
    "#    ''.join(i for i in text if ord(i)<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>mined_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Feb 17 21:23:20 +0000 2019</td>\n",
       "      <td>wojespn</td>\n",
       "      <td>2019-02-18 13:41:48.062467</td>\n",
       "      <td>112</td>\n",
       "      <td>“The more swings you get the more chances you ...</td>\n",
       "      <td>1097245133622382592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sun Feb 17 17:23:27 +0000 2019</td>\n",
       "      <td>wojespn</td>\n",
       "      <td>2019-02-18 13:41:48.062488</td>\n",
       "      <td>258</td>\n",
       "      <td>Atlanta Hawks GM Travis Schlenk on the franchi...</td>\n",
       "      <td>1097184766347145216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Feb 15 23:12:36 +0000 2019</td>\n",
       "      <td>wojespn</td>\n",
       "      <td>2019-02-18 13:41:48.062493</td>\n",
       "      <td>84</td>\n",
       "      <td>ESPN story on Tim Connelly’s contract extensio...</td>\n",
       "      <td>1096547855085592576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Feb 15 22:55:05 +0000 2019</td>\n",
       "      <td>wojespn</td>\n",
       "      <td>2019-02-18 13:41:48.062498</td>\n",
       "      <td>117</td>\n",
       "      <td>Connelly’s contract could’ve expired at end of...</td>\n",
       "      <td>1096543448868573184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Feb 15 22:54:14 +0000 2019</td>\n",
       "      <td>wojespn</td>\n",
       "      <td>2019-02-18 13:41:48.062502</td>\n",
       "      <td>522</td>\n",
       "      <td>Denver Nuggets president of basketball operati...</td>\n",
       "      <td>1096543232572444672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at   handle                   mined_at  \\\n",
       "0  Sun Feb 17 21:23:20 +0000 2019  wojespn 2019-02-18 13:41:48.062467   \n",
       "1  Sun Feb 17 17:23:27 +0000 2019  wojespn 2019-02-18 13:41:48.062488   \n",
       "2  Fri Feb 15 23:12:36 +0000 2019  wojespn 2019-02-18 13:41:48.062493   \n",
       "3  Fri Feb 15 22:55:05 +0000 2019  wojespn 2019-02-18 13:41:48.062498   \n",
       "4  Fri Feb 15 22:54:14 +0000 2019  wojespn 2019-02-18 13:41:48.062502   \n",
       "\n",
       "   retweet_count                                               text  \\\n",
       "0            112  “The more swings you get the more chances you ...   \n",
       "1            258  Atlanta Hawks GM Travis Schlenk on the franchi...   \n",
       "2             84  ESPN story on Tim Connelly’s contract extensio...   \n",
       "3            117  Connelly’s contract could’ve expired at end of...   \n",
       "4            522  Denver Nuggets president of basketball operati...   \n",
       "\n",
       "              tweet_id  \n",
       "0  1097245133622382592  \n",
       "1  1097184766347145216  \n",
       "2  1096547855085592576  \n",
       "3  1096543448868573184  \n",
       "4  1096543232572444672  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for i in range(5):\n",
    "#    print embiid[i]['text']\n",
    "#    print(' ')\n",
    "    \n",
    "#embiid_df = pd.DataFrame(embiid)\n",
    "james_df = pd.DataFrame(james)\n",
    "mccollum_df= pd.DataFrame(mccollum)\n",
    "durant_df = pd.DataFrame(durant)\n",
    "#lillard_df = pd.DataFrame(lillard)\n",
    "woj_df= pd.DataFrame(woj)\n",
    "shams_df = pd.DataFrame(shams)\n",
    "\n",
    "#tweets = pd.concat([embiid_df, james_df, mccollum_df,durant_df,lillard_df], axis=0)\n",
    "\n",
    "#tweets = pd.concat([embiid_df,lillard_df, james_df], axis=0)\n",
    "tweets = pd.concat([woj_df,durant_df], axis=0)\n",
    "\n",
    "\n",
    "#tweets['text'] = tweets['text'].apply(remove_non_ascii)\n",
    "#tweets['text'] = tweets['text'].astype(str)\n",
    " \n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"Atlanta Hawks GM Travis Schlenk on the franchise's rebuild, Trae Young/Luka Doncic draft night trade, his years at the ground floor of Warriors dynasty, working in Pat Riley's film room and much more. The Woj Pod: https://t.co/DK66e9yPpf\"\n",
      " u'ESPN story on Tim Connelly\\u2019s contract extension with the revitalized Denver Nuggets. https://t.co/UnEQgHGlV1'\n",
      " u'Connelly\\u2019s contract could\\u2019ve expired at end of this season, but discussions with Nuggets president Josh Kroenke had been ongoing in recent weeks and culminated with a new deal, league sources said. https://t.co/1V8RgCfFCi'\n",
      " u'Denver Nuggets president of basketball operations Tim Connelly, architect of one of the NBA\\u2019s most impressive recent rebuilds, has agreed to a contract extension, league sources tell ESPN.'\n",
      " u'Ferry has worked with the Pels in a part-time capacity as a consultant, but his most recent work as GM for the Atlanta Hawks was viewed league-wide as a clinic on how to reshape a roster and franchise without bottoming out into the lottery. https://t.co/QLdMayrqCI'\n",
      " u'New Orleans has now named Danny Ferry the interim GM, league sources tell ESPN.'\n",
      " u'New Orleans consultant Danny Ferry -- former Hawks and Cavs GM -- is the likely choice for Pelicans interim GM, league sources tell ESPN. Details are still being finalized.']\n",
      "-------------------------------------------------------------------------------------\n",
      "[u'atlanta hawks gm travis schlenk on the franchise s rebuild trae young luka doncic draft night trade his years at the ground floor of warriors dynasty working in pat riley s film room and much more the woj pod url', u'espn story on tim connelly s contract extension with the revitalized denver nuggets url', u'connelly s contract could ve expired at end of this season but discussions with nuggets president josh kroenke had been ongoing in recent weeks and culminated with a new deal league sources said url', u'denver nuggets president of basketball operations tim connelly architect of one of the nba s most impressive recent rebuilds has agreed to a contract extension league sources tell espn', u'ferry has worked with the pels in a part time capacity as a consultant but his most recent work as gm for the atlanta hawks was viewed league wide as a clinic on how to reshape a roster and franchise without bottoming out into the lottery url', u'new orleans has now named danny ferry the interim gm league sources tell espn', u'new orleans consultant danny ferry former hawks and cavs gm is the likely choice for pelicans interim gm league sources tell espn details are still being finalized']\n"
     ]
    }
   ],
   "source": [
    "from textacy.preprocess import preprocess_text\n",
    "\n",
    "tweet_text = tweets['text'].values\n",
    "clean_text = [preprocess_text(t, fix_unicode=True, lowercase=True, \n",
    "                              no_urls=True, no_emails=True, no_phone_numbers=True,\n",
    "                              no_currency_symbols=True, no_punct=True, no_accents=True)\n",
    "              for t in tweet_text]\n",
    "\n",
    "print tweet_text[1:8]\n",
    "print '-------------------------------------------------------------------------------------'\n",
    "print clean_text[1:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [From, name, Joel, Hulu, Live, Sports, Embiid,...\n",
       "1    [Stars, live, Recruit, your, Star, team, #NBAL...\n",
       "2                                      [https, CCpzig]\n",
       "3    [Lolololololololololololololololololololololol...\n",
       "4    [honor, part, stringers, against, real, starte...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#def remove_pattern(input_txt, pattern):\n",
    "#    r = re.findall(pattern, input_txt)\n",
    "#    for i in r:\n",
    "#        input_txt = re.sub(i, '', input_txt)\n",
    "#        \n",
    "#    return input_txt    \n",
    "\n",
    "#tweets['text'] = np.vectorize(remove_pattern)(tweets['text'], \"@[\\w]*\")\n",
    "#tweets['text'] = tweets['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "#tweets['text'] = tweets['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "#tweets['text'] = tweets['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "\n",
    "#tokenized_tweet = tweets['text'].apply(lambda x: x.split())\n",
    "#tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531705539359\n"
     ]
    }
   ],
   "source": [
    "y = tweets['handle'].map(lambda x: 1 if x == 'wojespn' else 0).values\n",
    "print max(pd.Series(y).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5488, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#Vectorizing with TF-IDF Vectorizer and creating X matrix\n",
    "tfv = TfidfVectorizer(ngram_range=(2,4), max_features=2000)\n",
    "X = tfv.fit_transform(clean_text).todense()\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': array([1.00000e-05, 1.12332e-05, ..., 8.90215e-01, 1.00000e+00])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression()\n",
    "LogisticRegression(solver='lbfgs')\n",
    "\n",
    "params = {'penalty': ['l1', 'l2'], 'C':np.logspace(-5,0,100)}\n",
    "#Grid searching to find optimal parameters for Logistic Regression\n",
    "gs = GridSearchCV(lr, param_grid=params, cv=10, verbose=1)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'C': 1.0}\n",
      "0.9092565597667639\n"
     ]
    }
   ],
   "source": [
    "print gs.best_params_\n",
    "print gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9092367675800727\n",
      "0.5317055393586005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(LogisticRegression(), X, y, cv=10)\n",
    "\n",
    "print accuracies.mean()\n",
    "print 1-y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Proba_durant</th>\n",
       "      <th>Proba_woj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.720873</td>\n",
       "      <td>0.279127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037079</td>\n",
       "      <td>0.962921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Proba_durant  Proba_woj\n",
       "0      0.720873   0.279127\n",
       "1      0.037079   0.962921"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = LogisticRegression(penalty='l2',C=1.0)\n",
    "estimator.fit(X,y)\n",
    "\n",
    "# Prep our source as TfIdf vectors\n",
    "source_test = [\n",
    "    \"That’s was AMAZING!!! Got me so hyped I wanted to be in it too! Kudos\",\n",
    "    \"ESPN story on Tim Connelly’s contract extension with the revitalized Denver Nuggets.\"\n",
    "]\n",
    "Xtest = tfv.transform(source_test)\n",
    "pd.DataFrame(estimator.predict_proba(Xtest), columns=[\"Proba_durant\", \"Proba_woj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "#cv = CountVectorizer()\n",
    "\n",
    "for i in range(5):\n",
    "    print tweets[i]['text']\n",
    "    print(' ')\n",
    "    \n",
    "#tweets['text'] = tweets['text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "#list_tweets =tweets['text'].tolist()\n",
    "#[x.encode('UTF8') for x in list_tweets]\n",
    "\n",
    "\n",
    "#vectorizer = TfidfVectorizer(ngram_range=(2,4), stop_words='english',sublinear_tf=True)\n",
    "\n",
    "\n",
    "\n",
    "#count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "#                            ngram_range=(2, 4), max_df=1.0, min_df=1, max_features=200)\n",
    "\n",
    "#count_train = count_vec.fit(list_tweets)\n",
    "#bag_of_words = count_vec.transform(list_tweets)\n",
    "#print(\"\\nEvery 3rd feature:\\n{}\".format(count_vec.get_feature_names()))\n",
    "\n",
    "#print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\n",
    "\n",
    "\n",
    "\n",
    "#print list_tweets\n",
    "\n",
    "#X = vectorizer.fit_transform(list_tweets)\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "#print(X.shape)\n",
    "\n",
    "\n",
    "#summaries = \"\".join(tweets['text'])\n",
    "#X_train_counts = cv.fit_transform(embiid_df.text)\n",
    "#X_train_counts.shape\n",
    "#cv.vocabulary_\n",
    "\n",
    "#print summaries\n",
    "\n",
    "#ngrams_summaries = vectorizer.build_analyzer()(X)\n",
    "\n",
    "#Counter(ngrams_summaries).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 200\n",
      "{u'thanks lot': 167, u'best luck': 8, u'kingjames favorite': 94, u'stay tuned': 157, u'girls ain loyal': 54, u'lt lt lt lt': 123, u'wearefamily https': 185, u'lot chronicles': 115, u'hard work': 81, u'don worry': 43, u'great time': 67, u'facts https': 45, u'just like': 90, u'working hard': 191, u'lol just': 109, u'good game': 58, u'allez les': 3, u'boss https': 14, u'love https': 118, u'damedolla frontpagemusic': 31, u'really appreciate': 142, u'ripcitygoon simzadam_jr': 146, u'hard kind': 80, u'let going': 99, u'good day': 57, u'time https': 174, u'gonna miss': 56, u'happy new': 78, u'parking lot chronicles': 135, u'happy thanksgiving': 79, u'new pulluppod': 132, u'world https': 192, u'joel embiid nbavote': 87, u'appreciate love': 7, u'love thanks': 119, u'album confirmed': 2, u'day https': 35, u'lil homie': 104, u'good morning': 61, u'big time': 12, u'good time': 62, u'look like': 113, u'good win': 63, u'certainly https': 21, u'month https': 129, u'super bowl': 160, u'don think': 40, u'know https': 95, u'love guys': 117, u'theland striveforgreatness': 171, u'check new': 23, u'big fella': 10, u'let know': 101, u'great team': 66, u'thanks support': 170, u'win tonight': 188, u'speedy recovery': 154, u'game tonight': 51, u'way https': 183, u've challenged': 178, u'thanks bro': 163, u'make sure': 126, u'rwtw striveforgreatness': 149, u'kdtrey5 think': 92, u'big bro': 9, u'day homie': 34, u'high schools': 84, u'pre order': 138, u'ripcitygoon simzadam_jr king_chaney chrisjamke': 148, u'game today': 50, u'good https': 59, u'thanks bruh': 164, u'getting better': 52, u'trust process': 176, u'just don': 88, u'don wanna': 41, u'year https': 194, u'yup https': 199, u'bruh https': 20, u'bro rt': 17, u'post game': 137, u'amen https': 5, u'trusttheprocess https': 177, u'frontpagemusic https': 48, u'lt lt lt': 122, u'major digital': 125, u'damedolla https': 33, u'lol love': 110, u'nbavote https': 131, u'star game': 156, u'respect program': 143, u've seen': 180, u'striveforgreatness https': 159, u'bra https': 15, u'great win': 68, u'ad https': 0, u'lmao https': 105, u'lol rt': 111, u'thanks https': 166, u'feel like': 47, u'high school': 83, u'week https': 186, u've challenged students': 179, u'brother https': 18, u'congrats brother': 29, u'smh https': 153, u'lol http': 107, u'thanks love': 168, u'yea https': 193, u'going https': 55, u'don miss': 39, u'youtube https': 198, u'just got': 89, u'let https': 100, u'like https': 102, u'congrats bro': 28, u'team win': 161, u'man just': 127, u'bruh bruh': 19, u'music platforms': 130, u'look good': 112, u'things https': 173, u'great game': 65, u'kubball kucmb': 96, u'weallfromafrica http': 184, u'happy bday': 73, u'lol got': 106, u'springhillent https': 155, u'happy day': 75, u'training camp': 175, u'great conversation': 64, u'fantasy experience': 46, u'pulluppod schultz_report': 140, u'theprocess https': 172, u'latest pulluppod': 97, u'happy gday': 76, u'embiid nbavote': 44, u'didn know': 36, u'girls ain': 53, u'parkrose roosevelt': 136, u'simzadam_jr king_chaney': 151, u'yes sir': 196, u'work hard': 189, u'hit line': 85, u'kdtrey5 favorite': 91, u'gt gt': 69, u'don make': 38, u'luck year': 124, u'real https': 141, u'don know': 37, u'shot clock': 150, u'years ago': 195, u'thank https': 162, u'lt lt': 121, u'love bro': 116, u'couple weeks': 30, u'bigalknows waldorfsfinest': 13, u'happy monday': 77, u'damedolla frontpagemusic https': 32, u'les bleus': 98, u'looking forward': 114, u'coming soon': 26, u'work hard kind': 190, u'striveforgreatness http': 158, u'confirmed https': 27, u'good luck': 60, u'appreciate https': 6, u'big homie': 11, u'come https': 24, u'new year': 133, u'king_chaney chrisjamke': 93, u'pretty good': 139, u'happy birthday': 74, u'coming https': 25, u'lol https': 108, u'don want': 42, u'thanks man': 169, u'challenged students': 22, u'bro https': 16, u'simzadam_jr king_chaney chrisjamke': 152, u'ain loyal': 1, u'love ya': 120, u'win fellas': 187, u'thanks fam': 165, u'ripcitygoon simzadam_jr king_chaney': 147, u'wait https': 181, u'man love': 128, u'waldorfsfinest staxxfifth': 182, u'allez les bleus': 4, u'ripcity https': 145, u'parking lot': 134, u'game https': 49, u'gt gt gt gt': 71, u'ykwtii https': 197, u'guy https': 72, u'joel embiid': 86, u'lil bro': 103, u'gt gt gt': 70, u'right https': 144, u'hello https': 82}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\n",
    "print count_train.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
